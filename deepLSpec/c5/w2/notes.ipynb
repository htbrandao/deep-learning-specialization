{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Word embedings\n",
    "\n",
    "After defining ou dictionary, and one-hot-encoding our words, we'll create a featurized representation.\n",
    "\n",
    "![](media/Screenshot.png)\n",
    "\n",
    "More features can be added.\n",
    "\n",
    "For a word $w$ and a amount $f$ of features, our vector $w$ will $f$-dimensiional.\n",
    "\n",
    "We can find correlation between the features:\n",
    "\n",
    "![](media/Screenshot(1).png)\n",
    "\n",
    "![](media/Screenshot(2).png)\n",
    "\n",
    "![](media/Screenshot(3).png)\n",
    "\n",
    "![](media/Screenshot(4).png)\n",
    "\n",
    "![](media/Screenshot(5).png)\n",
    "\n",
    "![](media/Screenshot(6).png)\n",
    "\n",
    "$w: argmax_w(sim(e_w, e_{king} - e_{man} + e_{woman}))$\n",
    "\n",
    "![](media/Screenshot(7).png)\n",
    "\n",
    "$sim(u, v) = cos(u, v)$\n",
    "\n",
    "![](media/Screenshot(8).png)\n",
    "\n",
    "[...] \"And all of these things can be learned just by running a word embedding learning algorithm on the large text corpus. It can spot all of these patterns by itself, just by running from very large bodies of text.\"\n",
    "\n",
    "![](media/Screenshot(9).png)\n",
    "\n",
    "![](media/Screenshot(10).png)\n",
    "\n",
    "Where $e_{6257}$ is the **embeding** of our previous one-hot vector $o_{6257}$\n",
    "\n",
    "Note that,\n",
    "\n",
    "$E * i_j = e_j$\n",
    "\n",
    "## ## Learning word embedings\n",
    "\n",
    "![](media/Screenshot(11).png)\n",
    "\n",
    "![](media/Screenshot(12).png)\n",
    "\n",
    "![](media/Screenshot(13).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Word2Vec skip-gram\n",
    "\n",
    "(Could be useful to know what is a [n-gram](https://en.wikipedia.org/wiki/N-gram))\n",
    "\n",
    "$Context$, $window$ (range) and $target$.\n",
    "\n",
    "We want to learn a mapping: $c \\rightarrow t$\n",
    "\n",
    "![](media/Screenshot(15).png)\n",
    "\n",
    "![](media/Screenshot(14).png)\n",
    "\n",
    "![](media/Screenshot(16).png)\n",
    "\n",
    "Where $Å·_i$ is $vocab size$-dimensional.\n",
    "\n",
    "## ## Negative Sampling\n",
    "\n",
    "Context - Word - Target\n",
    "\n",
    "![](media/Screenshot(17).png)\n",
    "\n",
    "![](media/Screenshot(18).png)\n",
    "\n",
    "The words with $0$ on $target$ are our negative examples.\n",
    "\n",
    "For $k$ times, choose words from our chorpus and assign their target related to the $context$ word as $0$.\n",
    "\n",
    "Create a supervised learning, as such:\n",
    "\n",
    "$L: (context, word) \\rightarrow target$\n",
    "\n",
    "![](media/Screenshot(19).png)\n",
    "\n",
    "![](media/Screenshot(20).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/rique/Downloads/Screenshot(20).png' -> 'media/Screenshot(20).png'\r\n"
     ]
    }
   ],
   "source": [
    "!cp -v /home/rique/Downloads/Screenshot\\(20\\).png media/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
