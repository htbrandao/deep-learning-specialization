{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Word embedings\n",
    "\n",
    "After defining ou dictionary, and one-hot-encoding our words, we'll create a featurized representation (obtained using a previous model).\n",
    "\n",
    "![](media/Screenshot.png)\n",
    "\n",
    "More features can be added.\n",
    "\n",
    "For a word $w$ and a amount $f$ of features, our vector $w$ will $f$-dimensiional.\n",
    "\n",
    "We can find correlation between the features:\n",
    "\n",
    "![](media/Screenshot(1).png)\n",
    "\n",
    "![](media/Screenshot(2).png)\n",
    "\n",
    "![](media/Screenshot(3).png)\n",
    "\n",
    "![](media/Screenshot(4).png)\n",
    "\n",
    "![](media/Screenshot(5).png)\n",
    "\n",
    "![](media/Screenshot(6).png)\n",
    "\n",
    "$w: argmax_w(sim(e_w, e_{king} - e_{man} + e_{woman}))$\n",
    "\n",
    "![](media/Screenshot(7).png)\n",
    "\n",
    "$sim(u, v) = cos(u, v)$\n",
    "\n",
    "![](media/Screenshot(8).png)\n",
    "\n",
    "[...] \"And all of these things can be learned just by running a word embedding learning algorithm on the large text corpus. It can spot all of these patterns by itself, just by running from very large bodies of text.\"\n",
    "\n",
    "![](media/Screenshot(9).png)\n",
    "\n",
    "![](media/Screenshot(10).png)\n",
    "\n",
    "Where $e_{6257}$ is the **embeding** of our previous one-hot vector $o_{6257}$\n",
    "\n",
    "Note that,\n",
    "\n",
    "$E * i_j = e_j$\n",
    "\n",
    "## ## Learning word embedings\n",
    "\n",
    "![](media/Screenshot(11).png)\n",
    "\n",
    "![](media/Screenshot(12).png)\n",
    "\n",
    "![](media/Screenshot(13).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Word2Vec skip-gram\n",
    "\n",
    "(Could be useful to know what is a [n-gram](https://en.wikipedia.org/wiki/N-gram))\n",
    "\n",
    "$Context$, $window$ (range) and $target$.\n",
    "\n",
    "For this word $w$, what is the next word? We want to learn a mapping: $c \\rightarrow t$\n",
    "\n",
    "Ex.:\n",
    "\n",
    "context: \"orange\" $\\rightarrow$ target: \"juice\"\n",
    "\n",
    "![](media/Screenshot(15).png)\n",
    "\n",
    "![](media/Screenshot(14).png)\n",
    "\n",
    "So, our only parameter will be $\\theta_t$.\n",
    "\n",
    "![](media/Screenshot(16).png)\n",
    "\n",
    "Where $Å·_i$ is $vocab size$-dimensional.\n",
    "\n",
    "## ## Negative Sampling\n",
    "\n",
    "Wre are going to predict if given two words, are they a context-target pair?\n",
    "\n",
    "![](media/Screenshot(17).png)\n",
    "\n",
    "![](media/Screenshot(18).png)\n",
    "\n",
    "The words with $0$ on $target$ are our negative examples.\n",
    "\n",
    "For $k$ times, choose words from our chorpus and assign their target related to the $context$ word as $0$.\n",
    "\n",
    "Create a supervised learning, as such:\n",
    "\n",
    "$L: (context, word) \\rightarrow target$\n",
    "\n",
    "![](media/Screenshot(19).png)\n",
    "\n",
    "![](media/Screenshot(20).png)\n",
    "\n",
    "![](media/Screenshot(21).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## GloVe (global vectors) word vectors\n",
    "\n",
    "Another approach to learn word embedings.\n",
    "\n",
    "![](media/Screenshot(22).png)\n",
    "\n",
    "Learn vectors such that their inner product tells how often words appear together.\n",
    "\n",
    "The $Cost$:\n",
    "\n",
    "![](media/Screenshot(23).png)\n",
    "\n",
    "Assume $log (0) = 0$.\n",
    "\n",
    "Note that the $f(x_{i,j})$ weight acts as as 'disposer' of non correlated words. Since $\\forall x, 0 * x = 0 $\n",
    "\n",
    "[...] \"this means the sum is sum only over the pairs of words that have co-occurred at least once in that context-target relationship\"\n",
    "\n",
    "![](media/Screenshot(24).png)\n",
    "\n",
    "- $\\theta_i, e_j$ are simetric.\n",
    "\n",
    "- $e^{(final)}_w = \\frac{e_w + \\theta_w}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Applications\n",
    "\n",
    "## ### Sentiment classification\n",
    "\n",
    "![](media/Screenshot(24).png)\n",
    "\n",
    "But this is prone to false positives!\n",
    "\n",
    "Now, using a $many-to-one$ RNN:\n",
    "\n",
    "![](media/Screenshot(25).png)\n",
    "\n",
    "## ### Debiasing\n",
    "\n",
    "![](media/Screenshot(26).png)\n",
    "\n",
    "![](media/Screenshot(27).png) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
