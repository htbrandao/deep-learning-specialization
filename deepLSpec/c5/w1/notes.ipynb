{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5: Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/Selection_001.png)\n",
    "\n",
    "Example:\n",
    "\n",
    "![](media/Selection_002.png)\n",
    "\n",
    "Where $x^{(i)<t>}$ representes the vector corresponding to the **$t$-esm** word on the **$i$-esm** input example.\n",
    "\n",
    "When dealing with text/words data, we need to one-hot encode it, meaning we also need a dictionary vector, with size = number of words (in alphabetical order).\n",
    "\n",
    "![](media/Selection_003.png)\n",
    "\n",
    "Pay attention that each vector will only have a $1$ on the correspondent position of it's word on the dictionary, and all other values will be $0$.\n",
    "\n",
    "For words not in the vocabulary, we assign it as '$unknown$' word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Recurrent Neural Network\n",
    "\n",
    "![](media/Selection_005.png)\n",
    "\n",
    "## ## Foward step\n",
    "\n",
    "Model example:\n",
    "\n",
    "For each time step $t$ the activation of $x^{<t>}$ will be passed on to $x^{<t+1>}$\n",
    "\n",
    "![](media/Selection_006.png)\n",
    "\n",
    "Note that $T_x$ and $T_y$ may have the **same lenght**, but that's not true for every case.\n",
    "\n",
    "It's also a good practice to introduce a initial ($t = 0$) input example, made of $0$'s:\n",
    "\n",
    "![](media/Selection_007.png)\n",
    "\n",
    "Note that, so far, the prediction $ŷ_n$ only uses the activation $a^{<n-1>}$, thus being unidirectional.\n",
    "\n",
    "![](media/Selection_008.png)\n",
    "\n",
    "\n",
    "Keep in mind that there are \"two moviments\" happening on each layer, one *horizontal* for finding $a^{<t>}$, and another *vertical* for classifying $x^{<t>}$.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "The activation functions for solving $a$ and $y$ are usually distinct functions!\n",
    "</div>\n",
    "\n",
    "Then,\n",
    "\n",
    "![](media/Selection_010.png)\n",
    "\n",
    "Whe can rewrite it:\n",
    "\n",
    "![](media/Selection_012.png)\n",
    "\n",
    "and,\n",
    "\n",
    "![](media/Selection_013.png)\n",
    "\n",
    "## ## Back step\n",
    "\n",
    "Remember the foward step:\n",
    "\n",
    "![](media/Selection_014.png)\n",
    "\n",
    "We'll use this $Loss$ equation:\n",
    "\n",
    "![](media/Selection_015.png)\n",
    "\n",
    "And the $Cost$ we'll be the sum ($\\sum$) of the $L_i$'s.\n",
    "\n",
    "Then,\n",
    "\n",
    "![](media/Selection_017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should give it a try to write down how the backprop equations would be like.\n",
    "\n",
    "You don't need - but would be good - to figure out each one exactly, but try to write then, at least, as a matter of the chain rule.\n",
    "\n",
    "e.g.:\n",
    "\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial ŷ^{<t>}}$:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial ŷ^{<t>}} = \\frac{-y}{ŷ^{<t>}} + \\frac{(1 - y)}{(1 - ŷ^{<t>})}$\n",
    "\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial a^{<t>}}$:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial a^{<t>}} = \\frac{\\partial \\mathcal{L}}{\\partial ŷ^{<t>}} * \\frac{\\partial \\mathcal{ŷ^{<t>}}}{\\partial a^{<t>}}$\n",
    "\n",
    "Where,\n",
    "\n",
    "$ŷ^{<t>} = g(W_{ya} * a^{<t>} + b_y)$\n",
    "\n",
    "Then,\n",
    "\n",
    "*Suppose $g(x)$ is the $sigmoid$ function, $\\frac{\\partial \\mathcal{g}}{\\partial x} = g(x)*(1 - g(x))$. In this case, we have a composite function, $g(f(x))$, where $f(x) = w * x + b$.*\n",
    "\n",
    "$\\frac{\\partial \\mathcal{ŷ^{<t>}}}{\\partial a^{<t>}} = \\frac{\\partial \\mathcal{y}}{\\partial g} * \\frac{\\partial \\mathcal{g}}{\\partial a}$\n",
    "\n",
    "$= g(W_{ya} * a^{<t>} + b_y)*(1 - g(W_{ya} * a^{<t>} + b_y)) * \\frac{\\partial \\mathcal{(W_{ya} * a^{<t>} + b_y)}}{\\partial a^{<t>}}$\n",
    "\n",
    "$= g(W_{ya} * a^{<t>} + b_y)*(1 - g(W_{ya} * a^{<t>} + b_y)) * W_{ya}$\n",
    "\n",
    "- And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## RNN Architectures\n",
    "\n",
    "- Many to many; many to one, one to one:\n",
    "\n",
    "![](media/Selection_018.png)\n",
    "\n",
    "- One to many:\n",
    "\n",
    "Note that, often, $ŷ^{<t>}$ will be fed to $ŷ^{<t+1>}$.\n",
    "\n",
    "![](media/Selection_019.png)\n",
    "\n",
    "- Many to many ($T_x = T_y$):\n",
    "\n",
    "Previously seen.\n",
    "\n",
    "- Many to many ($T_x \\ne T_y$):\n",
    "\n",
    "![](media/Selection_020.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Sequence Generation\n",
    "\n",
    "## ### Language moldel\n",
    "\n",
    "It's job is, given any sentence, to tell you the what is the probability $P(sentence)$ of that input being a certain sentence.\n",
    "\n",
    "![](media/Selection_021.png)\n",
    "\n",
    "$<EOS>$: End Of Sentence.\n",
    "\n",
    "$<UNK>$: Unknown token/word.\n",
    "\n",
    "We'll set up the inputs $x^{<t>} = y^{<t-1>}$.\n",
    "\n",
    "The first step:\n",
    "\n",
    "![](media/Selection_022.png)\n",
    "\n",
    "[...] \"But what $a^{<1>}$ does is it will make a $softmax$ prediction to try to figure out what is the probability of the first words $y$. And so that's going to be $ŷ^{<1>}$. So what this step does is really, it has a soft max it's trying to predict. What is the probability of any word in the dictionary?\" **For each word** in our corpus.\n",
    "\n",
    "We propagate $a^{<1>}$ to the next step, and try to guess the next word. But we give it the correct word:\n",
    "\n",
    "$y^{<1>} = x^{<2>} =$ **Cats**.\n",
    "\n",
    "Now, it shuould calculate the prbability of the next word ($ŷ^{<2>}$) **given the fact that the previous word is \"Cats\"**.\n",
    "\n",
    "Then, predict $ŷ^{<3>}$, given that $y^{<1>}$ and $y^{<2>}$ are given\n",
    "\n",
    "![](media/Screenshot(1).png)\n",
    "\n",
    "And finally, trying to predit $<EOS>$ given all previous inputs\n",
    "\n",
    "![](media/Screenshot(2).png)\n",
    "\n",
    "Each step of the RNN will look at a set of the previous words, learning to predict one word a time.\n",
    "\n",
    "For the loss and cost functions:\n",
    "\n",
    "![](media/Screenshot(3).png)\n",
    "\n",
    "## ## Sampling novel sequences\n",
    "\n",
    "After training time, we could sample from the model\n",
    "\n",
    "![](media/Screenshot(4).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Vanishing gradients with RNNs\n",
    "\n",
    "For a long sentece, it'll be hard for the error to back propagate until the input that might be early in the sequence, and modify the related weights.\n",
    "\n",
    "![](media/Screenshot(5).png)\n",
    "\n",
    "Exploding gradients will usualy lead to $NaN$s, which will be easy to spot.\n",
    "\n",
    "## ### Gated Recurrent Unit (GRU)\n",
    "\n",
    "Without GRU:\n",
    "\n",
    "![](media/Screenshot(8).png)\n",
    "\n",
    "With GRU:\n",
    "\n",
    "![](media/Screenshot(6).png)\n",
    "\n",
    "We have the following:\n",
    "\n",
    "- $-1 < tanh : $ (~c)$< 1$\n",
    "\n",
    "- $0 < sigmoid : \\Gamma_u < 1$\n",
    "\n",
    "Then,\n",
    "\n",
    "- $\\Gamma_u \\approx 0 \\Rightarrow c^{<t>} = c^{<t-1>}$, i.e: keep\n",
    "\n",
    "- $\\Gamma_u \\approx 1 \\Rightarrow c^{<t>} = $~$c^{<t>}$, i.e: update\n",
    "\n",
    "\n",
    "Note that all of the these will have the same dimmensions:\n",
    "\n",
    "![](media/Screenshot(7).png)\n",
    "\n",
    "$\\Gamma_r$ tells how relevant $c^{<t-1>}$ is to computing ~$c^{<t>}$\n",
    "\n",
    "Why $\\Gamma_r$? Because it's a good practice, so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## LSTM\n",
    "\n",
    "![](media/Screenshot(9).png)\n",
    "\n",
    "![](media/Screenshot(10).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Bidirectional RNN\n",
    "\n",
    "BRNNs will allow information regarding $ŷ^{<t>}$ to be fed from the future.\n",
    "\n",
    "![](media/Screenshot(11).png)\n",
    "\n",
    "Each cell $t$ will use $a^{\\leftarrow<t+1>}$ backward activation, which will be obained using $a^{\\leftarrow<t+2>}$, which will be obained using $a^{\\leftarrow<t+...>}$ and so on.\n",
    "\n",
    "Thus, suffering influence from all the tokens in the setence who come before and after it.\n",
    "\n",
    "![](media/Screenshot(12).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Deep RNN\n",
    "\n",
    "Try to think of it as a \"fully connected\" RNN.\n",
    "\n",
    "![](media/Screenshot(13).png)\n",
    "\n",
    "![](media/Screenshot(14).png)\n",
    "\n",
    "![](media/Screenshot(15).png)\n",
    "\n",
    "The evaluation blocks can also be GRUs or LSTMs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
