{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 5: Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/Selection_001.png)\n",
    "\n",
    "Example:\n",
    "\n",
    "![](media/Selection_002.png)\n",
    "\n",
    "Where $x^{(i)<t>}$ representes the vector corresponding to the **$t$-esm** word on the **$i$-esm** input example.\n",
    "\n",
    "When dealing with text/words data, we need to one-hot encode it, meaning we also need a dictionary vector, with size = number of words (in alphabetical order).\n",
    "\n",
    "![](media/Selection_003.png)\n",
    "\n",
    "Pay attention that each vector will only have a $1$ on the correspondent position of it's word on the dictionary, and all other values will be $0$.\n",
    "\n",
    "For words not in the vocabulary, we assign it as '$unknown$' word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Recurrent Neural Network\n",
    "\n",
    "![](media/Selection_005.png)\n",
    "\n",
    "## ## Foward step\n",
    "\n",
    "Model example:\n",
    "\n",
    "For each time step $t$ the activation of $x^{<t>}$ will be passed on to $x^{<t+1>}$\n",
    "\n",
    "![](media/Selection_006.png)\n",
    "\n",
    "Note that $T_x$ and $T_y$ may have the **same lenght**, but that's not true for every case.\n",
    "\n",
    "It's also a good practice to introduce a initial ($t = 0$) input example, made of $0$'s:\n",
    "\n",
    "![](media/Selection_007.png)\n",
    "\n",
    "Note that, so far, the prediction $ŷ_n$ only uses the activation $a^{<n-1>}$, thus being unidirectional.\n",
    "\n",
    "![](media/Selection_008.png)\n",
    "\n",
    "\n",
    "Keep in mind that there are \"two moviments\" happening on each layer, one *horizontal* for finding $a^{<t>}$, and another *vertical* for classifying $x^{<t>}$.\n",
    "\n",
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "The activation functions for solving $a$ and $y$ are usually distinct functions!\n",
    "</div>\n",
    "\n",
    "Then,\n",
    "\n",
    "![](media/Selection_010.png)\n",
    "\n",
    "Whe can rewrite it:\n",
    "\n",
    "![](media/Selection_012.png)\n",
    "\n",
    "and,\n",
    "\n",
    "![](media/Selection_013.png)\n",
    "\n",
    "## ## Back step\n",
    "\n",
    "Remember the foward step:\n",
    "\n",
    "![](media/Selection_014.png)\n",
    "\n",
    "We'll use this $Loss$ equation:\n",
    "\n",
    "![](media/Selection_015.png)\n",
    "\n",
    "And the $Cost$ we'll be the sum ($\\sum$) of the $L_i$'s.\n",
    "\n",
    "Then,\n",
    "\n",
    "![](media/Selection_017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should give it a try to write down how the backprop equations would be like.\n",
    "\n",
    "You don't need - but would be good - to figure out each one exactly, but try to write then, at least, as a matter of the chain rule.\n",
    "\n",
    "e.g.:\n",
    "\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial ŷ^{<t>}}$:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial ŷ^{<t>}} = \\frac{-y}{ŷ^{<t>}} + \\frac{(1 - y)}{(1 - ŷ^{<t>})}$\n",
    "\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial a^{<t>}}$:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial a^{<t>}} = \\frac{\\partial \\mathcal{L}}{\\partial ŷ^{<t>}} * \\frac{\\partial \\mathcal{ŷ^{<t>}}}{\\partial a^{<t>}}$\n",
    "\n",
    "Where,\n",
    "\n",
    "$ŷ^{<t>} = g(W_{ya} * a^{<t>} + b_y)$\n",
    "\n",
    "Then,\n",
    "\n",
    "*Suppose $g(x)$ is the $sigmoid$ function, $\\frac{\\partial \\mathcal{g}}{\\partial x} = g(x)*(1 - g(x))$. In this case, we have a composite function, $g(f(x))$, where $f(x) = w * x + b$.*\n",
    "\n",
    "$\\frac{\\partial \\mathcal{ŷ^{<t>}}}{\\partial a^{<t>}} = \\frac{\\partial \\mathcal{y}}{\\partial g} * \\frac{\\partial \\mathcal{g}}{\\partial a}$\n",
    "\n",
    "$= g(W_{ya} * a^{<t>} + b_y)*(1 - g(W_{ya} * a^{<t>} + b_y)) * \\frac{\\partial \\mathcal{(W_{ya} * a^{<t>} + b_y)}}{\\partial a^{<t>}}$\n",
    "\n",
    "$= g(W_{ya} * a^{<t>} + b_y)*(1 - g(W_{ya} * a^{<t>} + b_y)) * W_{ya}$\n",
    "\n",
    "- And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## RNN Architectures\n",
    "\n",
    "- Many many; many to one, one to one:\n",
    "\n",
    "![](media/Selection_018.png)\n",
    "\n",
    "- One to many:\n",
    "\n",
    "Note that, often, $ŷ^{<t>}$ will be fed to $ŷ^{<t+1>}$.\n",
    "\n",
    "![](media/Selection_019.png)\n",
    "\n",
    "- Many to many ($T_x = T_y$):\n",
    "\n",
    "Previously seen.\n",
    "\n",
    "- Many to many ($T_x \\ne T_y$):\n",
    "\n",
    "![](media/Selection_020.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Sequence Generation\n",
    "\n",
    "## ### Language moldel\n",
    "\n",
    "It's job is, given any sentence, to tell you the what is the probability $P(sentence)$ of that input being a certain sentence.\n",
    "\n",
    "![](media/Selection_021.png)\n",
    "\n",
    "![](media/Selection_022.png)\n",
    "\n",
    "![](media/Selection_023.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/rique/Pictures/Selection_021.png' -> 'media/Selection_021.png'\r\n"
     ]
    }
   ],
   "source": [
    "!cp -vf /home/rique/Pictures/Selection_021.png media/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
