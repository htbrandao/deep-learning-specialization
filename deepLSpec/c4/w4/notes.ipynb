{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 4: Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./media/Selection_008.png)\n",
    "\n",
    "Which we could translate to: Face recognition is the process to apply $1:k$ face verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # One shot learning\n",
    "\n",
    "![](./media/Selection_001.png)\n",
    "\n",
    "Whenever you need to assert for a new person, just add that picture to the database, because $d$ will give you how far the real person is from a picture from the database.\n",
    "\n",
    "Thus, being **verified**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Siamese Network\n",
    "\n",
    "Start by transforming the input image into a feature vector:\n",
    "\n",
    "![](./media/Selection_002.png)\n",
    "\n",
    "Which, we'll call $f(x^{(1)})$\n",
    "\n",
    "Same for another image:\n",
    "\n",
    "![](./media/Selection_003.png)\n",
    "\n",
    "$f(x^{(2)})$\n",
    "\n",
    "This process will be called **encoding**.\n",
    "\n",
    "With encoded vectors, we are able to calculate their's distance:\n",
    "\n",
    "![](./media/Selection_004.png)\n",
    "\n",
    "We need our NN to learn parameters so that:\n",
    "\n",
    "- If $x^{(i)}, x^{(i)}$ are the same person, $d$ is small;\n",
    "- If $x^{(i)}, x^{(i)}$ are **not** the same person, $d$ is **large**;\n",
    "\n",
    "One way to assure this learning, is the Triplet Loss\n",
    "\n",
    "## ## Triplet Loss\n",
    "\n",
    "To apply it, we need to compare images, to have bounds to judge wich is 'close' and which is 'far' from the desired person.\n",
    "\n",
    "The input image will be called Anchor (A), the close image will be referred as Positive (P) and the distant Negative (N).\n",
    "\n",
    "![](./media/Selection_005.png)\n",
    "\n",
    "Doing some algebra, we end with the following\n",
    "\n",
    "${||f(A) - f(P)||}^2 - {||f(A) - f(N)||}^2 <= 0$\n",
    "\n",
    "Note that it satisfies a trivial solution, where\n",
    "\n",
    "$0 - 0 <= 0$\n",
    "\n",
    "For that, we add a margin parameter alpha ($\\alpha$):\n",
    "\n",
    "${||f(A) - f(P)||}^2 - {||f(A) - f(N)||}^2 + \\alpha <= 0$\n",
    "\n",
    "Now, we define the Loss function:\n",
    "\n",
    "![](./media/Selection_006.png)\n",
    "\n",
    "And the Cost function as:\n",
    "\n",
    "$J = \\sum_{i=1}^{m} L(A^{(i)}, P^{(i)}, N^{(i)})$\n",
    "\n",
    "For your training set, you need multiple pictures for each person, so you have anchors and positive images for each one.\n",
    "\n",
    "![](./media/Selection_007.png)\n",
    "\n",
    "## ## Binary Classification\n",
    "\n",
    "This ia an alternative to the Triplet Loss.\n",
    "\n",
    "Rather stopping with the encoding, they will be feeded into a logistic regression unit\n",
    "\n",
    "![](./media/Selection_009.png)\n",
    "\n",
    "The *green* underlined part could be substituted by another form of calculating their distance (eg: difference/sum).\n",
    "\n",
    "One good practice is to have the encoding for each person pre-computed, and then do the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Style Transfer\n",
    "\n",
    "![](./media/Selection_011.png)\n",
    "\n",
    "We need to look at the features extracted by each layers. What are they learning?\n",
    "\n",
    "![](./media/Selection_012.png)\n",
    "\n",
    "Check the activation for some units in some layers. Notice how each one tends to respont to totally different features:\n",
    "\n",
    "![](./media/Selection_013.png)\n",
    "\n",
    "## ## Cost Function\n",
    "\n",
    "We define it as\n",
    "\n",
    "$J(G) = \\alpha * J_{content}(C, G) + \\beta * J_{style}(S, G)$,\n",
    "\n",
    "where $\\alpha,\\beta$ are hyperparameters and *C* is Content, *S* is Style and *G* is the generated output.\n",
    "\n",
    "![](./media/Selection_014.png)\n",
    "\n",
    "Note that, here, gradient descend is updating the pixel values from the random generated image, which will be our output.\n",
    "\n",
    "## ### Content Cost Function\n",
    "\n",
    "![](./media/Selection_016.png)\n",
    "\n",
    "$J_{content}(C, G) = \\frac{1}{2} * ||a^{[l](C)]} - a^{[l](G)]}||^2$\n",
    "\n",
    "\n",
    "## ### Style Cost Function\n",
    "\n",
    "\"What we need to do is define the style as the correlation between activations across different channels in this layer L activation\"\n",
    "\n",
    "![](./media/Selection_017.png)\n",
    "\n",
    "![](./media/Selection_018.png)\n",
    "\n",
    "[...] \"if we use the degree of correlation between channels as a measure of the style, then what you can do is measure the degree to which in your generated image, this first channel is correlated or uncorrelated with the second channel and that will tell you in the generated image how often this type of vertical texture occurs or doesn't occur with this orange-ish tint and this gives you a measure of how similar is the style of the generated image to the style of the input style image\"\n",
    "\n",
    "We will be computing a Style matrix:\n",
    "\n",
    "![](./media/Selection_019.png)\n",
    "\n",
    "![](./media/Selection_020.png)\n",
    "\n",
    "$J_{style}(S, G) = $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/home/rique/Pictures/Selection_018.png' -> 'media/Selection_018.png'\r\n"
     ]
    }
   ],
   "source": [
    "!cp -v /home/rique/Pictures/Selection_018.png media/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
