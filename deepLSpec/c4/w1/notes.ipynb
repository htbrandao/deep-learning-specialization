{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 4: Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Model \n",
    "\n",
    "Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. \n",
    "\n",
    "**Exercise**: Complete the function below. \n",
    "\n",
    "The model below should:\n",
    "\n",
    "- create placeholders\n",
    "- initialize parameters\n",
    "- forward propagate\n",
    "- compute the cost\n",
    "- create an optimizer\n",
    "\n",
    "Finally you will create a session and run a for loop  for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. [Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)\n",
    "\n",
    "#### Adam Optimizer\n",
    "You can use `tf.train.AdamOptimizer(learning_rate = ...)` to create the optimizer.  The optimizer has a `minimize(loss=...)` function that you'll call to set the cost function that the optimizer will minimize.\n",
    "\n",
    "For details, check out the documentation for [Adam Optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)\n",
    "\n",
    "#### Random mini batches\n",
    "If you took course 2 of the deep learning specialization, you implemented `random_mini_batches()` in the \"Optimization\" programming assignment. This function returns a list of mini-batches. It is already implemented in the `cnn_utils.py` file and imported here, so you can call it like this:\n",
    "```Python\n",
    "minibatches = random_mini_batches(X, Y, mini_batch_size = 64, seed = 0)\n",
    "```\n",
    "(You will want to choose the correct variable names when you use it in your code).\n",
    "\n",
    "#### Evaluating the optimizer and cost\n",
    "\n",
    "Within a loop, for each mini-batch, you'll use the `tf.Session` object (named `sess`) to feed a mini-batch of inputs and labels into the neural network and evaluate the tensors for the optimizer as well as the cost.  Remember that we built a graph data structure and need to feed it inputs and labels and use `sess.run()` in order to get values for the optimizer and cost.\n",
    "\n",
    "You'll use this kind of syntax:\n",
    "```\n",
    "output_for_var1, output_for_var2 = sess.run(\n",
    "                                                fetches=[var1, var2],\n",
    "                                                feed_dict={var_inputs: the_batch_of_inputs,\n",
    "                                                           var_labels: the_batch_of_labels}\n",
    "                                                )\n",
    "```\n",
    "* Notice that `sess.run` takes its first argument `fetches` as a list of objects that you want it to evaluate (in this case, we want to evaluate the optimizer and the cost).  \n",
    "* It also takes a dictionary for the `feed_dict` parameter.  \n",
    "* The keys are the `tf.placeholder` variables that we created in the `create_placeholders` function above.  \n",
    "* The values are the variables holding the actual numpy arrays for each mini-batch.  \n",
    "* The sess.run outputs a tuple of the evaluated tensors, in the same order as the list given to `fetches`. \n",
    "\n",
    "For more information on how to use sess.run, see the documentation [tf.Sesssion#run](https://www.tensorflow.org/api_docs/python/tf/Session#run) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Convolution\n",
    "\n",
    "[What is an intuitive explanation for convolution?](https://www.quora.com/What-is-an-intuitive-explanation-for-convolution)\n",
    "\n",
    "[Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Edge detection\n",
    "\n",
    "We'll apply a filter, running over the image matrix.\n",
    "\n",
    "The following filter (kernel) is used for vertical edge detection:\n",
    "\n",
    "![](media/ed.png)\n",
    "\n",
    "(For each iteration, this could be represented as: $line_{from\\_picture} * column_{from\\_filter}.T$)\n",
    "\n",
    "Why it works:\n",
    "\n",
    "![](media/ed2.png)\n",
    "\n",
    "[...] \"a vertical edge is where there are bright pixels on the left, you do not care that much what is in the middle and dark pixels on the right.\"\n",
    "\n",
    "![](media/ed3.png)\n",
    "\n",
    "___\n",
    "**Example:**\n",
    "\n",
    "![](media/ed4.png)\n",
    "\n",
    "___\n",
    "**Example:**\n",
    "\n",
    "![](media/ed5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Padding\n",
    "\n",
    "Default convolving will lead to lose information on the image's border/edges.\n",
    "\n",
    "![](media/padd.png)\n",
    "\n",
    "So, we could add/use padding. Which will increase the relevance of the image's borders/edges pixels and preserve the output dimmension equals to the input's.\n",
    "\n",
    "![](media/padd2.png)\n",
    "\n",
    "___\n",
    "Forms of implementing padding:\n",
    "\n",
    "![](media/padd3.png)\n",
    "\n",
    "$padding = \\frac{f - 1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Strided convolutions\n",
    "\n",
    "*(Note that the **default** stride value is `1`)*\n",
    "\n",
    "Works as a `step`, $(i,j)$-wise.\n",
    "\n",
    "With a padding of $p$ and a stride of $s$:\n",
    "\n",
    "\n",
    "![](media/stri.png)\n",
    "\n",
    "$i.e.:$\n",
    "\n",
    "______\n",
    "\n",
    "![](media/stri3.png)\n",
    "\n",
    "___\n",
    "\n",
    "*(For non-integer values, round down (floor) the value.)*\n",
    "\n",
    "The kernel product only happens when the multiplication is possible:\n",
    "\n",
    "![](media/stri2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ \n",
    "**Math:**\n",
    "\n",
    "He then explains about this twist and turn,\n",
    "\n",
    "![](media/stri4.png)\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <b>Disclaimer:</b>\n",
    "    <img src='media/stri5.png'></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Achieving this result is just a matter of performin the following:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[[ 3  4  5]\n",
      " [ 1  0  2]\n",
      " [-1  9  7]]\n",
      "\n",
      "flipping horizontally: \u001b[1mv * anti_id\u001b[0m\n",
      "\n",
      "flipping vertically: \u001b[1manti_id * v\u001b[0m\n",
      "\n",
      "output:\n",
      "[[ 7  9 -1]\n",
      " [ 2  0  1]\n",
      " [ 5  4  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([[3, 4, 5], [1, 0, 2], [-1, 9, 7]])\n",
    "\n",
    "print('input:\\n{}'.format(v))\n",
    "\n",
    "# flipped identity matrix\n",
    "anti_id = np.array([[0, 0, 1],[0, 1, 0],[1, 0, 0]])\n",
    "\n",
    "# flip horizontally: v * anti_id\n",
    "print('\\nflipping horizontally: \\033[1mv * anti_id\\033[0m')\n",
    "first_step = np.dot(v, anti_id)\n",
    "\n",
    "# flip vertically: anti_id * v\n",
    "print('\\nflipping vertically: \\033[1manti_id * v\\033[0m')\n",
    "second_step = np.dot(anti_id, first_step)\n",
    "\n",
    "print('\\noutput:\\n{}'.format(second_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### Convolutions over volume (RGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll apply convolutions, likewise, on each layer of the rgb channels.\n",
    "\n",
    "![](media/rgb.png)\n",
    "\n",
    "We could apply filters that only work on a single channel, zeroing the other layers.\n",
    "\n",
    "Convolving on volumes will allow us to operate on RGB pictures and detect more complex edges, like horizontal edges, or angled edges.\n",
    "\n",
    "![](media/rgb2.png)\n",
    "\n",
    "![](media/rgb3.png)\n",
    "\n",
    "![](media/rgb4.png)\n",
    "\n",
    "![](media/rgb5.png)\n",
    "\n",
    "![](media/rgb6.png)\n",
    "\n",
    "![](media/rgb7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #### l-convolutional layer of a convolution network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If layer '$l$' is a convolution layer:\n",
    "\n",
    "\n",
    "$f^{[l]}$ = filter size\n",
    "\n",
    "$p^{[l]}$ = padding\n",
    "\n",
    "$s^{[l]}$ = stride\n",
    "\n",
    "$n_c^{[l]}$ = number of filters\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the size of each filter will be,\n",
    "\n",
    "$f^{[l]}$ x $f^{[l]}$ x numberOfChannels\n",
    "\n",
    "i.e.:\n",
    "\n",
    "$f^{[l]}$ x $f^{[l]}$ x $n_{channels}^{[l-1]}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights (all filters put together) have dimension:\n",
    "\n",
    "$f^{[l]}$ x $f^{[l]}$ x $n_{channels}^{[l-1]}$ x $n_{channels}^{[l]}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias will be of shape\n",
    "\n",
    "$1$ x $n_c^{[l]}$\n",
    "\n",
    "$e.g.: (1, 1, 1,$ ...$, n_c^{[l]})$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dimension:\n",
    "\n",
    "$n_{height}^{[l-1]}$ x $n_{width}^{[l-1]}$ x $n_{channels}^{[l-1]}$\n",
    "___\n",
    "\n",
    "Output dimension:\n",
    "\n",
    "$n_{height}^{[l]}$ x $n_{width}^{[l]}$ x $n_{channels}^{[l]}$\n",
    "\n",
    "**where**,\n",
    "\n",
    "$n^{[l]} = floor([\\frac{n^{[l]}+2*p^{[l]}-f}{s^{[l]}} +1]$)\n",
    "\n",
    "if $height \\neq width$, then\n",
    "\n",
    "$n_{height}^{[l]} = floor([\\frac{n_{height}^{[l]}+2*p^{[l]}-f}{s^{[l]}} +1]$)\n",
    "\n",
    "**and**,\n",
    "\n",
    "$n_{width}^{[l]} = floor([\\frac{n_{width}^{[l]}+2*p^{[l]}-f}{s^{[l]}}+1]$)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation of the $l$-Layer for a single example:\n",
    "\n",
    "$a^{[l]} = n_{height}^{[l]}$ x $n_{width}^{[l]}$ x $n_{channels}^{[l]}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the activation of the entire layer $l$ for $m$ training examples:\n",
    "\n",
    "$A^{[l]} = n_{height}^{[l]}$ x $n_{width}^{[l]}$ x $n_{channels}^{[l]}$ x $m$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "![](media/ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Max pooling\n",
    "\n",
    "Select the highest value from an area (strided):\n",
    "\n",
    "![](media/mp.png)\n",
    "\n",
    "If this/these features are detected anywhere in this filter, then keep a high number to represent it, else, maybe this feature doesn't exist.\n",
    "\n",
    "*(Note that this layer has no parameters to pass to back propagation)*\n",
    "\n",
    "![](media/mp2.png)\n",
    "\n",
    "\n",
    "We could also use `average pooling` instead as an alternative to max pooling.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "![](media/mp3.png)\n",
    "\n",
    "![](media/mp4.png)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common architecture that we may find is:\n",
    "\n",
    "`conv -> pool -> conv -> pool -> fc -> fc -> softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Why convolutions?\n",
    "\n",
    "[...] \"I think there are two main advantages of convolutional layers over just using fully connected layers. And the advantages are parameter sharing and sparsity of connections\"\n",
    "\n",
    "- For starters, we save on the size of the $W^{[l]}$'s matrix:\n",
    "\n",
    "![](media/conv.png)\n",
    "\n",
    "- Parameter sharing:\n",
    "\n",
    "![](media/conv2.png)\n",
    "\n",
    "[...] \"And maybe you do have a dataset where you have the upper left-hand corner and lower right-hand corner have different distributions, so, they maybe look a little bit different but they might be similar enough, they're sharing feature detectors all across the image, works just fine.\"\n",
    "\n",
    "- Sparsity of connections:\n",
    "\n",
    "![](media/conv3.png)\n",
    "\n",
    "![](media/conv4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
