{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 4: Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Convolution\n",
    "\n",
    "[What is an intuitive explanation for convolution?](https://www.quora.com/What-is-an-intuitive-explanation-for-convolution)\n",
    "\n",
    "[Intuitively Understanding Convolutions for Deep Learning](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Edge detection\n",
    "\n",
    "We'll apply a filter, running over the image matrix.\n",
    "\n",
    "The following filter (kernel) is used for vertical edge detection:\n",
    "\n",
    "![](media/ed.png)\n",
    "\n",
    "(For each iteration, this could be represented as: $line_{from\\_picture} * column_{from\\_filter}.T$)\n",
    "\n",
    "Why it works:\n",
    "\n",
    "![](media/ed2.png)\n",
    "\n",
    "[...] \"a vertical edge is where there are bright pixels on the left, you do not care that much what is in the middle and dark pixels on the right.\"\n",
    "\n",
    "![](media/ed3.png)\n",
    "\n",
    "___\n",
    "**Example:**\n",
    "\n",
    "![](media/ed4.png)\n",
    "\n",
    "___\n",
    "**Example:**\n",
    "\n",
    "![](media/ed5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Padding\n",
    "\n",
    "Default convolving will lead to lose information on the image's border/edges.\n",
    "\n",
    "![](media/padd.png)\n",
    "\n",
    "So, we could add/use padding. Which will increase the relevance of the image's borders/edges pixels and preserve the output dimmension equals to the input's.\n",
    "\n",
    "![](media/padd2.png)\n",
    "\n",
    "___\n",
    "Forms of implementing padding:\n",
    "\n",
    "![](media/padd3.png)\n",
    "\n",
    "$padding = \\frac{f - 1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Strided convolutions\n",
    "\n",
    "*(Note that the **default** stride value is `1`)*\n",
    "\n",
    "Works as a `step`, $(i,j)$-wise.\n",
    "\n",
    "With a padding of $p$ and a stride of $s$:\n",
    "\n",
    "\n",
    "![](media/stri.png)\n",
    "\n",
    "$i.e.:$\n",
    "\n",
    "______\n",
    "\n",
    "![](media/stri3.png)\n",
    "\n",
    "___\n",
    "\n",
    "*(For non-integer values, round down (floor) the value.)*\n",
    "\n",
    "The kernel product only happens when the multiplication is possible:\n",
    "\n",
    "![](media/stri2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ \n",
    "**Math:**\n",
    "\n",
    "He then explains about this twist and turn,\n",
    "\n",
    "![](media/stri4.png)\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <b>Disclaimer:</b>\n",
    "    <img src='media/stri5.png'></img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Achieving this result is just a matter of performin the following:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[[ 3  4  5]\n",
      " [ 1  0  2]\n",
      " [-1  9  7]]\n",
      "\n",
      "flipping horizontally: \u001b[1mv * anti_id\u001b[0m\n",
      "\n",
      "flipping vertically: \u001b[1manti_id * v\u001b[0m\n",
      "\n",
      "output:\n",
      "[[ 7  9 -1]\n",
      " [ 2  0  1]\n",
      " [ 5  4  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([[3, 4, 5], [1, 0, 2], [-1, 9, 7]])\n",
    "\n",
    "print('input:\\n{}'.format(v))\n",
    "\n",
    "# flipped identity matrix\n",
    "anti_id = np.array([[0, 0, 1],[0, 1, 0],[1, 0, 0]])\n",
    "\n",
    "# flip horizontally: v * anti_id\n",
    "print('\\nflipping horizontally: \\033[1mv * anti_id\\033[0m')\n",
    "first_step = np.dot(v, anti_id)\n",
    "\n",
    "# flip vertically: anti_id * v\n",
    "print('\\nflipping vertically: \\033[1manti_id * v\\033[0m')\n",
    "second_step = np.dot(anti_id, first_step)\n",
    "\n",
    "print('\\noutput:\\n{}'.format(second_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### Convolutions over volume (RGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll apply convolutions, likewise, on each layer of the rgb channels.\n",
    "\n",
    "![](media/rgb.png)\n",
    "\n",
    "We could apply filters that only work on a single channel, zeroing the other layers.\n",
    "\n",
    "Convolving on volumes will allow us to operate on RGB pictures and detect more complex edges, like horizontal edges, or angled edges.\n",
    "\n",
    "![](media/rgb2.png)\n",
    "\n",
    "![](media/rgb3.png)\n",
    "\n",
    "![](media/rgb4.png)\n",
    "\n",
    "![](media/rgb5.png)\n",
    "\n",
    "![](media/rgb6.png)\n",
    "\n",
    "![](media/rgb7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #### l-convolutional layer of a convolution network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If layer '$l$' is a convolution layer:\n",
    "\n",
    "\n",
    "$f^{[l]}$ = filter size\n",
    "\n",
    "$p^{[l]}$ = padding\n",
    "\n",
    "$s^{[l]}$ = stride\n",
    "\n",
    "$n_c^{[l]}$ = number of filters\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the size of each filter will be,\n",
    "\n",
    "$f^{[l]}$ x $f^{[l]}$ x numberOfChannels\n",
    "\n",
    "i.e.:\n",
    "\n",
    "$f^{[l]}$ x $f^{[l]}$ x $n_{channels}^{[l-1]}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights (all filters put together) have dimension:\n",
    "\n",
    "$f^{[l]}$ x $f^{[l]}$ x $n_{channels}^{[l-1]}$ x $n_{channels}^{[l]}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias will be of shape\n",
    "\n",
    "$1$ x $n_c^{[l]}$\n",
    "\n",
    "$e.g.: (1, 1, 1,$ ...$, n_c^{[l]})$\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input dimension:\n",
    "\n",
    "$n_{height}^{[l-1]}$ x $n_{width}^{[l-1]}$ x $n_{channels}^{[l-1]}$\n",
    "___\n",
    "\n",
    "Output dimension:\n",
    "\n",
    "$n_{height}^{[l]}$ x $n_{width}^{[l]}$ x $n_{channels}^{[l]}$\n",
    "\n",
    "**where**,\n",
    "\n",
    "$n^{[l]} = floor([\\frac{n^{[l]}+2*p^{[l]}-f}{s^{[l]}} +1]$)\n",
    "\n",
    "if $height \\neq width$, then\n",
    "\n",
    "$n_{height}^{[l]} = floor([\\frac{n_{height}^{[l]}+2*p^{[l]}-f}{s^{[l]}} +1]$)\n",
    "\n",
    "**and**,\n",
    "\n",
    "$n_{width}^{[l]} = floor([\\frac{n_{width}^{[l]}+2*p^{[l]}-f}{s^{[l]}}+1]$)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation of the $l$-Layer for a single example:\n",
    "\n",
    "$a^{[l]} = n_{height}^{[l]}$ x $n_{width}^{[l]}$ x $n_{channels}^{[l]}$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the activation of the entire layer $l$ for $m$ training examples:\n",
    "\n",
    "$A^{[l]} = n_{height}^{[l]}$ x $n_{width}^{[l]}$ x $n_{channels}^{[l]}$ x $m$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "![](media/ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Max pooling\n",
    "\n",
    "Select the highest value from an area (strided):\n",
    "\n",
    "![](media/mp.png)\n",
    "\n",
    "If this/these features are detected anywhere in this filter, then keep a high number to represent it, else, maybe this feature doesn't exist.\n",
    "\n",
    "*(Note that this layer has no parameters to pass to back propagation)*\n",
    "\n",
    "![](media/mp2.png)\n",
    "\n",
    "\n",
    "We could also use `average pooling` instead as an alternative to max pooling.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "![](media/mp3.png)\n",
    "\n",
    "![](media/mp4.png)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common architecture that we may find is:\n",
    "\n",
    "`conv -> pool -> conv -> pool -> fc -> fc -> softmax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Why convolutions?\n",
    "\n",
    "[...] \"I think there are two main advantages of convolutional layers over just using fully connected layers. And the advantages are parameter sharing and sparsity of connections\"\n",
    "\n",
    "- For starters, we save on the size of the $W^{[l]}$'s matrix:\n",
    "\n",
    "![](media/conv.png)\n",
    "\n",
    "- Parameter sharing:\n",
    "\n",
    "![](media/conv2.png)\n",
    "\n",
    "[...] \"And maybe you do have a dataset where you have the upper left-hand corner and lower right-hand corner have different distributions, so, they maybe look a little bit different but they might be similar enough, they're sharing feature detectors all across the image, works just fine.\"\n",
    "\n",
    "- Sparsity of connections:\n",
    "\n",
    "![](media/conv3.png)\n",
    "\n",
    "![](media/conv4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
