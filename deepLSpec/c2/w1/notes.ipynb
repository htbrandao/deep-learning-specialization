{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2: Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Sets\n",
    "\n",
    "[...] 'take all the data you have and carve off some portion of it to  be your **training set**. Some portion of it to be your *hold-out cross validation set*, and *this is sometimes also called the development set*. And for brevity I'm just going to call this the **dev set**, but all of these terms mean roughly the same thing.'\n",
    "\n",
    "![split](./files/media/split.png)\n",
    "\n",
    "The **goal** of the dev set is that you're going to test different algorithms on it and see which algorithm works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Bias and variance\n",
    "\n",
    "*(If you never heard about the 'Bias-Variance Tradeoff, there's a great TLDR [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwj14r6clYbnAhUDG7kGHZVSChoQFjAAegQIARAB&url=https%3A%2F%2Fstats.stackexchange.com%2Fquestions%2F4284%2Fintuitive-explanation-of-the-bias-variance-tradeoff&usg=AOvVaw1A8Yka6B1y54v36zlsXn0S))*\n",
    "\n",
    "![bv](./files/media/bv.png)\n",
    "\n",
    "Example:\n",
    "\n",
    "![bv2](./files/media/bv2.png)\n",
    "\n",
    "(*About **Bayes Error**: See [here](https://www.cs.helsinki.fi/u/jkivinen/opetus/iml/2013/Bayes.pdf)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Basic recipe\n",
    "\n",
    "![rec](./files/media/rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Regularization\n",
    "\n",
    "[Regularization (mathematics) - Wikipedia](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwinjqyptYbnAhVKOKwKHVtsCdcQFjACegQIDRAG&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FRegularization_(mathematics)&usg=AOvVaw04AA1ClsGSf0abnrOMr_C2): \"In mathematics, statistics, and computer science, particularly in machine learning and inverse problems, regularization is the process of adding information in order to solve an ill-posed problem or to prevent overfitting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Weight Decay\n",
    "\n",
    "![wd](./files/media/wd.png)\n",
    "\n",
    "Note that, when computing the parameters update, we end up with\n",
    "\n",
    "$w^{[L]} = w^{[L]} * (1 - \\frac{alfa * lambd}m) - alfa * {\\partial w^{[L]}}$\n",
    "\n",
    "Since,\n",
    "\n",
    "(`alfa`, `lambd`, `m`) > 0\n",
    "\n",
    "Then,\n",
    "\n",
    "$(1 - \\frac{alfa * lambd}m) < 1$, \n",
    "\n",
    "And,\n",
    "\n",
    "$w^{[L]} * (1 - \\frac{alfa * lambd}m) < w^{[L]}$\n",
    "\n",
    "Which means that we are, first, performing a '*shrinking*' of $w^{[L]}$ and then updating it as we know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Why regularization reduces overfitting?\n",
    "\n",
    "![reg](./files/media/reg.png)\n",
    "\n",
    "[...] \"if you crank regularisation lambda to be really, really big,\n",
    "they'll be really incentivized to set the weight matrices W to be reasonably close to zero. So one piece of intuition is maybe it set the weight to be so close to zero for a lot of hidden units that's basically zeroing out a lot of the impact of these hidden units.\n",
    "And if that's the case, then this much simplified neural network becomes a much smaller neural network.\"\n",
    "\n",
    "![reg2](./files/media/reg2.png)\n",
    "\n",
    "[...] \"And so that will take you from this overfitting case much closer to the left to other high bias case. But hopefully there'll be an intermediate value of lambda that results in a result closer to this just right case in the middle. But the intuition is that by cranking up lambda to be really big they'll set W close to zero,\n",
    "which in practice this isn't actually what happens. We can think of it as zeroing out or at least reducing the impact of a lot of the hidden units so you end up with what might feel like a simpler network.\"\n",
    "\n",
    "![reg3](./files/media/reg3.png)\n",
    "\n",
    "Each 'disabled' hidden unit will have a much lower impact on data.\n",
    "\n",
    "If $lambd$ is too high, the behaviour of the network will be close to linear, which might result in a 'linear network'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Dropout regularization\n",
    "\n",
    "Randomly remove neurons from layers and train on example to see how it goes\n",
    "\n",
    "![drop](./files/media/drop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### Inverted dropout\n",
    "\n",
    "![drop2](./files/media/drop2.png)\n",
    "\n",
    "Where the vector `d` will point out which neurons to shut down.\n",
    "\n",
    "[...] \"for different training examples, you zero out different hidden units. And in fact, if you make multiple passes through the same training set, then on different pauses through the training set, you should randomly zero out different hidden units\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Data augmentation\n",
    "\n",
    "You might mirror, skew, zoom, apply little distortion to your data and generate more inputs whenever you cannot acquire new different data\n",
    "\n",
    "![aug](./files/media/aug.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Early stopping\n",
    "\n",
    "Plot both training error and dev set error curve.\n",
    "\n",
    "Find the latter point where both curves are close, and keep those parameters\n",
    "\n",
    "![stop](./files/media/stop.png)\n",
    "\n",
    "[...] \"And the advantage of early stopping is that running the gradient descent process just once, you get to try out values of small w, mid-size w, and large w, without needing to try a lot of values of the L2 regularization hyperparameter lambda.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Normalizing inputs\n",
    "\n",
    "![norm](./files/media/norm.png)\n",
    "\n",
    "Rember the relation between `std dev` and `variance:` $stdDev = variance^{1/2}$\n",
    "\n",
    "So,\n",
    "\n",
    "- variance: ${\\sigma^2}$\n",
    "\n",
    "- standard deviation: ${\\sigma}$\n",
    "\n",
    "\n",
    "Now we have a dataset whose ${\\mu} = {\\sigma} = 1$. It's now, by definition, a [normal distribution](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiS68HG_4rnAhVkH7kGHfEKDDAQFjAAegQIBBAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNormal_distribution&usg=AOvVaw044ZZPoqQRQ0hObRmquz6Z).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>TLDR:</b> Subtract the mean and divide by standard deviation: $x_{norm} = \\frac{x - \\mu}{\\sigma}$\n",
    "</div>\n",
    "\n",
    "![norm2](./files/media/norm2.png)\n",
    "\n",
    "Normalizing data will make it easir for gradient descent, achieving minimum $J(w, b)$ faster, speeding up training time and using less computing resources.\n",
    "\n",
    "![norm3](./files/media/norm3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Vanishing/exploding gradients\n",
    "\n",
    "Take this example:\n",
    "\n",
    "![vanex](./files/media/vanex.png)\n",
    "\n",
    "Then,\n",
    "\n",
    "$ŷ = W^{[L]} * W^{[L-1]} * x$\n",
    "\n",
    "Now, note that for `?` > 1, we'll have\n",
    "\n",
    "- Exploding: `gradients` -> $+∞$.\n",
    "\n",
    "Whereas for values 0 < `?` < 1,\n",
    "\n",
    "- Vanishing: `gradients` -> 0.\n",
    "\n",
    "What can lead us towards long training periods, since it'll be hard to optimize the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Weight initialization\n",
    "\n",
    "Normalize the random initialized weights.\n",
    "\n",
    "For a single neuron:\n",
    "\n",
    "![ini](./files/media/ini.png)\n",
    "*(this works for a ReLU activation)*\n",
    "\n",
    "[...] \"And this doesn't solve, but it definitely helps reduce the vanishing, exploding gradients problem, because it's trying to set each of the weight matrices w, you know, so that it's not too much\n",
    "bigger than 1 and not too much less than 1 so it doesn't explode or vanish too quickly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Numerical approximation\n",
    "\n",
    "![](./files/media/.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renamed '/home/f4119597/Downloads/Screenshot.png' -> 'files/media/ini.png'\r\n"
     ]
    }
   ],
   "source": [
    "!mv -v /home/f4119597/Downloads/Screenshot*.png files/media/ini.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
