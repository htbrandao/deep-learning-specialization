{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Course 2: Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Hyperparameters tuning\n",
    "\n",
    "[...] \"it's just hard to know in advance which ones turn out to be\n",
    "the really important hyperparameters for your application and sampling at random rather than in the grid shows that you are more richly\n",
    "exploring set of possible values for the most important hyperparameters, whatever they turn out to be.\"\n",
    "\n",
    "![](media/rand.png)\n",
    "\n",
    "[...] \"When you sample hyperparameters, another common practice is to use a coarse to fine sampling scheme\"\n",
    "\n",
    "![](media/rand2.png)\n",
    "\n",
    "[...] \"You can then sample more densely into smaller square. So this type of a coarse to fine search is also frequently used\"\n",
    "\n",
    "To sum up: Do **not** use grid search.\n",
    "\n",
    "We just have to be 'randomly smart'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**:\n",
    "\n",
    "Using a logarithmic scale:\n",
    "\n",
    "![](media/rand3.png)\n",
    "\n",
    "Since,\n",
    "\n",
    "$a = {log_{10}}^{0.0001}$\n",
    "\n",
    "$a = -4$, since $10^{-4} = 0.0001$\n",
    "\n",
    "and, \n",
    "\n",
    "$b = {log_{10}}^{1}$\n",
    "\n",
    "$b = 0$, since $10^{0} = 1$\n",
    "\n",
    "A random value will be $\\alpha = 10^r$, where r is in {a, b} = {-4, 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python,\n",
    "\n",
    "`r = -4 * np.random.rand()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "The neat trick is, instead of $\\beta$, we'll use $1-\\beta$.\n",
    "\n",
    "![](media/rand4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Watching over models\n",
    "\n",
    "![](media/bbs.png)\n",
    "\n",
    "Choose the approach that better fits your (computer) resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Batch Normalization\n",
    "\n",
    "[...] \"what batch norm does is it applies that normalization process not just to the input layer, but to the values even deep in some hidden layer in the neural network\"\n",
    "\n",
    "You will be normalizing intermediate values during the fowawrd propagation step.\n",
    "\n",
    "The activations $z^{[l](i)}$ will be normalized.\n",
    "\n",
    "$z_{norm}^{(i)} = \\frac{z^{(i)}}{\\sqrt{\\sigma^2 + \\epsilon}}$, we add $\\epsilon$ in case $\\sigma$ is zero.\n",
    "\n",
    "In case the distribution isn't normal ($\\mu = 0$, $\\sigma = 1$), we may calculate\n",
    "\n",
    "$ẑ^{(i)} = \\sqrt{\\gamma * z_{norm}^{(i)} + \\beta}$, where $\\gamma,\\beta$ are parameters from the model that will be updated.\n",
    "\n",
    "(`^` == `~`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling $\\gamma,\\beta$:\n",
    "\n",
    "![](media/btn.png)\n",
    "\n",
    "[...] \"But by choosing other values of gamma and beta, this allows you to make the hidden unit values have other means and variances as well.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "[...] \"For example, if you have a sigmoid activation function, you don't want your values to always be clustered here. You might want them to have a larger variance or have a mean that's different than 0, in order to better take advantage of the nonlinearity of the sigmoid function rather than have all your values be in just this linear  regime\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Fitting Batch Norm (BN) into a neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/btn2.png)\n",
    "\n",
    "![](media/btn3.png)\n",
    "\n",
    "![](media/btn4.png)\n",
    "\n",
    "![](media/btn5.png)\n",
    "\n",
    "[...] \"Because any constant you add will get cancelled out by the mean subtractions step.\"\n",
    "\n",
    "Then,\n",
    "\n",
    "![](media/btn6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[...] \"batch norm reduces the problem of the input values changing,\n",
    "it really causes these values to become more stable, so that the later layers of the neural network has more firm ground to stand on.\n",
    "\n",
    "And even though the input distribution changes a bit, it changes less, and what this does is, even as the earlier layers keep learning, the amounts that this forces the later layers to adapt to as early as layer changes is reduced or, if you will, **it weakens the coupling  between what the early layers parameters has to do and what the later layers parameters have to do.**\n",
    "\n",
    "And so it allows each layer of the network to learn by itself, a  little bit more independently of other layers, and this has the effect of speeding up of learning in the whole network\"\n",
    "\n",
    "![](media/btn7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that batch norm processes data a single mini-batch at a time.\n",
    "\n",
    "Wehere, $m$ is the number of examples on the mini-batch.\n",
    "\n",
    "![](media/btn8.png)\n",
    "\n",
    "[...] \"you keep a running average of the mu and the sigma squared that you're seeing for each layer as you train the neural network across different mini batches.\"\n",
    "\n",
    "![](media/btn9.png)\n",
    "\n",
    "Now, we are able to compute $z_{norm}$ and $ẑ$:\n",
    "\n",
    "![](media/btn10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Softmax Regression\n",
    "\n",
    "$C = $ # $classes$\n",
    "\n",
    "![](media/sft.png)\n",
    "\n",
    "be $x$ a possible class of $Y$,\n",
    "\n",
    "$ŷ = P(class|x)$,\n",
    "\n",
    "$ŷ.shape = (C, 1)$\n",
    "\n",
    "![](media/sft2.png)\n",
    "\n",
    "Note that $ŷ$ will be a vector of probabilities, which means that the sum of it's terms must be equal to $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{[L]} = w^{[L]} * a^{[L-1]} + b^{[L]}$\n",
    "\n",
    "Activation:\n",
    "\n",
    "$t = e^{z^{[L]}}$\n",
    "\n",
    "$a^{[L]} = \\frac{e^{[L]}}{\\sum_{i =1}^{n}t_i}$, where n is the number of classes (`C == shape[0]` of the vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "![](media/sft3.png)\n",
    "\n",
    "![](media/sft4.png)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "![](media/sft5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ### Loss\n",
    "\n",
    "![](media/sft6.png)\n",
    "\n",
    "So, even when the correct class isn't the one with the biggest probability, the loss function will be adjusted.\n",
    "\n",
    "And,\n",
    "\n",
    "## ### Cost\n",
    "\n",
    "$J = \\frac{1}{m} * \\sum_{i = 1}^{n} L(y^{(i)}, ŷ^{(i)})$\n",
    "\n",
    "$dz = \\frac{\\partial J}{\\partial z} = (\\frac{\\partial J}{\\partial z^{[L]}}) = \\frac{\\partial J}{\\partial a} * \\frac{\\partial a}{\\partial z} = ŷ - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Tensorflow\n",
    "\n",
    "([Tensorflow overview](https://www.tensorflow.org/overview?hl=en))\n",
    "\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_softmax_example():\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=5)\n",
    "    model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
