{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Course1: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing a linear regression, we want to find `ŷ`, for a given `y`. for that we try to find parameters `w` and `b` so that:\n",
    "\n",
    "    ŷ = w * x + b, ŷ ~= y\n",
    "    \n",
    "If we apply the sigmoid to this result, we shall have the probability of that occurrence.\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "    ŷ = sigmoid(w * x + b),  0 < ŷ < 1\n",
    "\n",
    "Hence, the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lreg](files/media/lreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Loss function & Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss_and_cost](files/media/loss_and_cost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the **cost function** is just an average from the **loss function** applied to our entire training set.\n",
    "\n",
    "We need just to try and minimize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use different loss functions.\n",
    "\n",
    "But, we need a cost function that is convex, so we may garantee it will have a global minimum point.\n",
    "\n",
    "![convex](files/media/convex.png)\n",
    "\n",
    "Then, we apply gradient descent to try and find that point, which will give us the smallest possible loss function value. Assuring we'll have optimized our loss function, and as a consequence, our model.\n",
    "\n",
    "![gd](files/media/gd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know/remember what is a gradient (from a function), maybe [THIS](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient) may help.\n",
    "\n",
    "Nonetheless, just try to remember that the gradient vector will point to the closest region with the highest value of the given function.\n",
    "\n",
    "In our case, our function is the **Loss function**. Hence, the higher the value, worse it is for me/you/everybody.\n",
    "\n",
    "We want to minimize it! So, instead of following it's gradient to the maximum region, we could follow the opposite direction of the gradient, and find it's region with the lowest value!\n",
    "\n",
    "E.g.: (ignoring `b` parameter)\n",
    "![gd_2d](files/media/gd_2d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Computation Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[...] \"the computations of a neural network are organized in terms of a forward pass or a `forward propagation` step, in which we compute the output of the neural network, **followed** by a backward pass or `back propagation` step, which we use to compute `gradients or compute derivatives`. The computation graph explains why it is organized this way.\"\n",
    "\n",
    "Almost as it were something like this:\n",
    "\n",
    "    input > foward prop > output > back prop > derivatives and gradients - \\/\n",
    "    /\\ - derivatives and gradients < back prop < output < foward prop < input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(IMHO, this chain of thought on the example he gives just reminds of a [callByName](https://stackoverflow.com/questions/13337338/call-by-name-vs-call-by-value-in-scala-clarification-needed) approach, where the function variables are evaluated only when needed)\n",
    "\n",
    "![cg1](files/media/cg1.png)\n",
    "\n",
    "Applying values to those variables (a, b, c):\n",
    "\n",
    "    step 1) a = 5, b = 3, c = 2\n",
    "\n",
    "    step 2) u = 6\n",
    "            v = 11\n",
    "\n",
    "    step 3) j = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "\n",
    "I really think that his explanation on derivatives is pedagogically good.\n",
    "\n",
    "I have a strong math background, and I actually got confused sometimes trying to follow his chain of thought, cause sometimes, there's a key component missing, and it usually is just a little bit more higher level math.\n",
    "\n",
    "But since we are going to deal with heavy Mathematics, I strongly advise for - at least - writting down main derivatives techniques. On time, you'll get acquainted and will have trouble forgetting them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a single entry of a training set:\n",
    "\n",
    "![back0](files/media/back0.png)\n",
    "\n",
    "    To derivate \"L\", remeber that:\n",
    "    \n",
    "    i) d(f + g)/dx = df/dx + dg/dx\n",
    "    \n",
    "    ii) d(f * g)/dx = (df/dx * g + f * dg/dx)\n",
    "\n",
    "![back1](files/media/back1.png)\n",
    "\n",
    "    Remeber:\n",
    "    \n",
    "    iii) d(f/g)/dx = (df/dx * g + f * dg/dx) / g²\n",
    "    \n",
    "    Given f(x), g(x):\n",
    "    \n",
    "    iv) df(g)/dx = f'(g) * g'(x)\n",
    "    \n",
    "    And/or just know that the derivative of the sigmoid function is equal to:\n",
    "                            sigmoid * (1 - sigmoid)\n",
    "\n",
    "![back2](files/media/back2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On 'm' examples training set:\n",
    "\n",
    "\n",
    "    Remeber:\n",
    "\n",
    "    Given a constant 'k':\n",
    "    v) d(k*f)/dx = k * df/dx\n",
    "\n",
    "    Remember that 'm' is the length of your training set.\n",
    "\n",
    "    Now, assume k = (1/m), which makes 'k' a constant:\n",
    "\n",
    "![loss_m](files/media/loss_m.png)\n",
    "\n",
    "    From the previous slide, we can see that:\n",
    "    \n",
    "                            dL/da = (-y/a) + (1-y)/(1-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
