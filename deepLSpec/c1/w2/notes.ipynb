{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course1: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Logistic Regression\n",
    "\n",
    "When doing a linear regression, we want to find `ŷ`, for a given `y`. for that we try to find parameters `w` and `b` so that:\n",
    "\n",
    "    ŷ = w * x + b, ŷ ~= y\n",
    "    \n",
    "If we apply the sigmoid to this result, we shall have the probability of that occurrence.\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "    ŷ = sigmoid(w * x + b),  0 < ŷ < 1\n",
    "\n",
    "Hence, the logistic regression.\n",
    "\n",
    "![lreg](files/media/lreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Loss function & Cost function\n",
    "\n",
    "![loss_and_cost](files/media/loss_and_cost.png)\n",
    "\n",
    "Note that the **cost function** is just an average from the **loss function** applied to our entire training set.\n",
    "\n",
    "We need just to try and minimize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Gradient Descent\n",
    "\n",
    "We could use different loss functions.\n",
    "\n",
    "But, we need a cost function that is convex, so we may garantee it will have a global minimum point.\n",
    "\n",
    "![convex](files/media/convex.png)\n",
    "\n",
    "Then, we apply gradient descent to try and find that point, which will give us the smallest possible loss function value. Assuring we'll have optimized our loss function, and as a consequence, our model.\n",
    "\n",
    "![gd](files/media/gd.png)\n",
    "\n",
    "If you don't know/remember what is a gradient (from a function), maybe [THIS](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient) may help.\n",
    "\n",
    "Nonetheless, just try to remember that the gradient vector will point to the closest region with the highest value of the given function.\n",
    "\n",
    "In our case, our function is the **Loss function**. Hence, the higher the value, worse it is for me/you/everybody.\n",
    "\n",
    "We want to minimize it! So, instead of following it's gradient to the maximum region, we could follow the opposite direction of the gradient, and find it's region with the lowest value!\n",
    "\n",
    "E.g.:\n",
    "\n",
    "(ignoring `b` parameter)\n",
    "\n",
    "![gd_2d](files/media/gd_2d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Computation Graph\n",
    "\n",
    "\n",
    "[...] \"the computations of a neural network are organized in terms of a forward pass or a `forward propagation` step, in which we compute the output of the neural network, **followed** by a backward pass or `back propagation` step, which we use to compute `gradients or compute derivatives`. The computation graph explains why it is organized this way.\"\n",
    "\n",
    "Almost as it were something like this:\n",
    "\n",
    "    input > foward prop > output > back prop > derivatives and gradients - \\/\n",
    "    /\\ - derivatives and gradients < back prop < output < foward prop < input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(IMHO, this chain of thought on the example he gives just reminds of a [callByName](https://stackoverflow.com/questions/13337338/call-by-name-vs-call-by-value-in-scala-clarification-needed) approach, where the function variables are evaluated only when needed)\n",
    "\n",
    "![cg1](files/media/cg1.png)\n",
    "\n",
    "Applying values to those variables (a, b, c):\n",
    "\n",
    "    step 1) a = 5, b = 3, c = 2\n",
    "\n",
    "    step 2) u = 6\n",
    "            v = 11\n",
    "\n",
    "    step 3) j = 33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following up on the previous slide, we'll try and compute the derivatives, from **right to left**:\n",
    "\n",
    "dJ/dv\n",
    "    \n",
    "    J(v) = 3v \n",
    "    dJ/dv = 3\n",
    "    \n",
    "dJ/da\n",
    "\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u\n",
    "    dJ/da = 3\n",
    "\n",
    "dJ/db\n",
    "\n",
    "    (since we are derivating in relation to b, c is constant)\n",
    "    c = 2\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u = 3a + 3(b*2) = 3a + 6b\n",
    "    dJ/db = 6\n",
    "\n",
    "dJ/du\n",
    "\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u = 3a + 3(bc) = 3a + 3bc\n",
    "    dJ/du = 3\n",
    "    \n",
    "dJ/dc\n",
    "\n",
    "    (since we are derivating in relation to c, b is constant)\n",
    "    b = 3\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u = 3a + 3(3c) = 3a + 9c\n",
    "    dJ/dc = 9\n",
    "\n",
    "\n",
    "\n",
    "dv/da\n",
    "\n",
    "    v = a + u\n",
    "    dv/da = 1\n",
    "    \n",
    "\n",
    "du/db\n",
    "\n",
    "    (since we are derivating in relation to b, c is constant)\n",
    "    c = 2\n",
    "    u = bc = 2b\n",
    "    du/db = 2\n",
    "    \n",
    "\n",
    "Which gives us:\n",
    "\n",
    "    dJ/dv => dv = 3\n",
    "    dJ/da => da = 3\n",
    "    dJ/db => db = 6\n",
    "    dJ/dc => dc = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: Note that we could use the chain rule (just like he does), since we could treat it as \n",
    "    \n",
    "    \n",
    "    j = 3v\n",
    "    v = a + u\n",
    "    \n",
    "    j(v) = 3(a + u)\n",
    "    \n",
    "    What would give us:\n",
    "    \n",
    "    dj/da = j'(v) * v' = 3 * 1 = 3\n",
    "\n",
    "But I honestly think that's more confusing    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(I really, really think that his intentions are good, but it's development gets quite confusing...)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a single entry of a training set (single training example):\n",
    "\n",
    "![back0](files/media/back0.png)\n",
    "\n",
    "`da`:\n",
    "\n",
    "    L(ŷ, y) = -(y * ln[ŷ] + (1 - y) * ln[1 - ŷ] )\n",
    "    assume ŷ = a\n",
    "    We are derivating in relation to `a`, then, assume `y` is a constant:\n",
    "    dL/da  = -[y * (1/a)] + [(1 - y) * (1 / 1 - a) * (-1)]\n",
    "    dL/da  = -y/a + ((1 - y) / (1 - a))\n",
    "    \n",
    "    Then,\n",
    "        da = -y/a + ((1 - y) / (1 - a))\n",
    "    \n",
    "`dz`:\n",
    "\n",
    "    All you need to do is:\n",
    "    1 - Replace the values on L(a, y)\n",
    "    2 - Derivate the sigmoid\n",
    "    3 - Remember the chain rule\n",
    "\n",
    "    [...]\n",
    "    \n",
    "    dL/dz = a - y\n",
    "    \n",
    "    Then,\n",
    "        dz = a - y\n",
    "\n",
    "![back1](files/media/back1.png)\n",
    "\n",
    "`dw1`, `dw2` and `db`:\n",
    "\n",
    "    dL/dw1 = dL/dz * dz/dw1 = (a - y) * dz/dw1 = (a - y) * x1 = x1 * \"dz\"\n",
    "    dL/dw1 => \"dw1\" = x1 * \"dz\"\n",
    "\n",
    "    dL/dw2 = dL/dz * dz/dw2 = (a - y) * dz/dw2 = (a - y) * x1 = x2 * \"dz\"\n",
    "    dL/dw2 => \"dw2\" = x1 * \"dz\"\n",
    "            \n",
    "    dL/db = dL/dz * dz/db = (a - y) * dz/db = (a - y) * 1 = 1 * \"dz\" = \"dz\"\n",
    "    dL/db => \"db\" = \"dz\"\n",
    "            \n",
    "![back2](files/media/back2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On 'm' examples (training set):\n",
    "\n",
    "We start `J`, `dw1`, `dw2` and `db` equals to `0`, because we will use them do accumulate their values and then take the average.\n",
    "\n",
    "(Or we can update them on each iteration. e.g.: dw1 /= m)\n",
    "\n",
    "![loss_m](files/media/loss_m.png)\n",
    "\n",
    "\n",
    "Assuming our training examples have a dimmension of 2, hence parameters are only w1, w2:\n",
    "\n",
    "![loss_m-2](files/media/loss_m-2.png)\n",
    "\n",
    "And we update them on each iteration:\n",
    "\n",
    "    J /= m\n",
    "    dw1 /= m\n",
    "    dw2 /= m\n",
    "    db /= m\n",
    "\n",
    "Finally, we take **one step** towards the gradient descent direction, updating our parameters:\n",
    "\n",
    "    w1 = w1 - alfa * dw1\n",
    "    w2 = w2 - alfa * dw2\n",
    "    b = b - alfa * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
