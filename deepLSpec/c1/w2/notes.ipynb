{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Course1: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Loss function & Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![loss_and_cost](files/media/loss_and_cost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the **cost function** is just an average from the **loss function** applied to our entire training set.\n",
    "\n",
    "We need just to try and minimize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't know/remember what is a gradient (from a function), maybe [THIS](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient) may help.\n",
    "\n",
    "Nonetheless, just try to remember that the gradient vector will point to the closest region with the highest value of the given function.\n",
    "\n",
    "In our case, our function is the **Loss function**. Hence, the higher the value, worse it is for me/you/everybody.\n",
    "\n",
    "We want to minimize it! So, instead of following it's gradient to the maximum region, we could follow the opposite direction of the gradient, and find it's region with the lowest value!\n",
    "\n",
    "![gd](files/media/gd.png)\n",
    "![gd_2d](files/media/gd_2d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a single entry of a training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0](files/media/Screenshot.png)\n",
    "\n",
    "    To derivate \"L\", remeber that:\n",
    "    \n",
    "    i) d(f + g)/dx = df/dx + dg/dx\n",
    "    \n",
    "    ii) d(f * g)/dx = (df/dx * g + f * dg/dx)\n",
    "\n",
    "![1](files/media/Screenshot(1).png)\n",
    "\n",
    "    To derivate \"a\", remeber that:\n",
    "    \n",
    "    iii) d(f/g)/dx = (df/dx * g + f * dg/dx) / gÂ²\n",
    "    \n",
    "    And/or just know that the derivative of the sigmoid function is equal to:\n",
    "                            sigmoid * (1 - sigmoid)\n",
    "\n",
    "![2](files/media/Screenshot(2).png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On 'm' examples training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Remeber that:\n",
    "    \n",
    "    Given a constant 'k':\n",
    "    iv) d(k*f)/dx = k * df/dx\n",
    "    \n",
    "    Remember that 'm' is the length of your training set.\n",
    "    \n",
    "    Now, assume k = (1/m), which makes 'k' a constant:\n",
    "\n",
    "![loss_m](files/media/loss_m.png)\n",
    "\n",
    "    From the previous slide, we can see that:\n",
    "    \n",
    "                            dL/da = (-y/a) + (1-y)/(1-a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
