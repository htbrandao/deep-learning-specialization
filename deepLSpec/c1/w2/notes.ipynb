{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 1: Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Disclaimer: `log` == `ln`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Logistic Regression\n",
    "\n",
    "When doing a linear regression, we want to find `ŷ`, for a given `y`. for that we try to find parameters `w` and `b` so that:\n",
    "\n",
    "    ŷ = w * x + b, ŷ ~= y\n",
    "    \n",
    "If we apply the sigmoid to this result, we shall have the probability of that occurrence.\n",
    "\n",
    "Then, we have:\n",
    "\n",
    "    ŷ = sigmoid(w * x + b),  0 < ŷ < 1\n",
    "\n",
    "Hence, the logistic regression.\n",
    "\n",
    "![lreg](files/media/lreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Loss function & Cost function\n",
    "\n",
    "![loss_and_cost](files/media/loss_and_cost.png)\n",
    "\n",
    "Note that the **cost function** is just an average from the **loss function** applied to our entire training set.\n",
    "\n",
    "We need just to try and minimize it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Gradient Descent\n",
    "\n",
    "We could use different loss functions.\n",
    "\n",
    "But, we need a cost function that is convex, so we may garantee it will have a global minimum point.\n",
    "\n",
    "![convex](files/media/convex.png)\n",
    "\n",
    "Then, we apply gradient descent to try and find that point, which will give us the smallest possible loss function value. Assuring we'll have optimized our loss function, and as a consequence, our model.\n",
    "\n",
    "![gd](files/media/gd.png)\n",
    "\n",
    "If you don't know/remember what is a gradient (from a function), maybe [THIS](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient) may help.\n",
    "\n",
    "Nonetheless, just try to remember that the gradient vector will point to the closest region with the highest value of the given function.\n",
    "\n",
    "In our case, our function is the **Loss function**. Hence, the higher the value, worse it is for me/you/everybody.\n",
    "\n",
    "We want to minimize it! So, instead of following it's gradient to the maximum region, we could follow the opposite direction of the gradient, and find it's region with the lowest value!\n",
    "\n",
    "E.g.:\n",
    "\n",
    "(ignoring `b` parameter)\n",
    "\n",
    "![gd_2d](files/media/gd_2d.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Computation Graph\n",
    "\n",
    "\n",
    "[...] \"the computations of a neural network are organized in terms of a forward pass or a `forward propagation` step, in which we compute the output of the neural network, **followed** by a backward pass or `back propagation` step, which we use to compute `gradients or compute derivatives`. The computation graph explains why it is organized this way.\"\n",
    "\n",
    "An interation would be close to something like this:\n",
    "\n",
    "    > input\n",
    "    \n",
    "    >> foward propagation\n",
    "    >>> output\n",
    "    >>>> back propagation\n",
    "    \n",
    "    >> foward propagation\n",
    "    >>> output\n",
    "    >>>> back propagation\n",
    "    \n",
    "    [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(IMHO, this chain of thought on the example he gives just reminds of a [callByName](https://stackoverflow.com/questions/13337338/call-by-name-vs-call-by-value-in-scala-clarification-needed) approach, where the function variables are evaluated only when needed)\n",
    "\n",
    "![cg1](files/media/cg1.png)\n",
    "\n",
    "Applying values to those variables (a, b, c):\n",
    "\n",
    "    step 1) a = 5, b = 3, c = 2\n",
    "\n",
    "    step 2) u = 6\n",
    "            v = 11\n",
    "\n",
    "    step 3) j = 33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following up on the previous slide, we'll try and compute the derivatives, from **right to left**:\n",
    "\n",
    "dJ/dv\n",
    "    \n",
    "    J(v) = 3v \n",
    "    dJ/dv = 3\n",
    "    \n",
    "dJ/da\n",
    "\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u\n",
    "    dJ/da = 3\n",
    "\n",
    "dJ/db\n",
    "\n",
    "    (since we are derivating in relation to b, c is constant)\n",
    "    c = 2\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u = 3a + 3(b*2) = 3a + 6b\n",
    "    dJ/db = 6\n",
    "\n",
    "dJ/du\n",
    "\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u = 3a + 3(bc) = 3a + 3bc\n",
    "    dJ/du = 3\n",
    "    \n",
    "dJ/dc\n",
    "\n",
    "    (since we are derivating in relation to c, b is constant)\n",
    "    b = 3\n",
    "    J(v) = 3v = 3(a + u) = 3a + 3u = 3a + 3(3c) = 3a + 9c\n",
    "    dJ/dc = 9\n",
    "\n",
    "\n",
    "\n",
    "dv/da\n",
    "\n",
    "    v = a + u\n",
    "    dv/da = 1\n",
    "    \n",
    "\n",
    "du/db\n",
    "\n",
    "    (since we are derivating in relation to b, c is constant)\n",
    "    c = 2\n",
    "    u = bc = 2b\n",
    "    du/db = 2\n",
    "    \n",
    "\n",
    "Which gives us:\n",
    "\n",
    "    dJ/dv => dv = 3\n",
    "    dJ/da => da = 3\n",
    "    dJ/db => db = 6\n",
    "    dJ/dc => dc = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS: Note that we could use the chain rule (just like he does), since we could treat it as:\n",
    "    \n",
    "    \n",
    "    j = 3v\n",
    "    v = a + u\n",
    "    \n",
    "    j(v) = 3(a + u)\n",
    "    \n",
    "    What would give us:\n",
    "    \n",
    "    dj/da = j'(v) * v' = 3 * 1 = 3\n",
    "\n",
    "But I honestly think that's more confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(I really, really think that his intentions are good, but it's development gets quite confusing...)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On a single single training example:\n",
    "\n",
    "![back0](files/media/back0.png)\n",
    "\n",
    "`da`:\n",
    "\n",
    "    L(ŷ, y) = -(y * ln[ŷ] + (1 - y) * ln[1 - ŷ] )\n",
    "    assume ŷ = a\n",
    "    We are derivating in relation to `a`, then, assume `y` is a constant:\n",
    "    dL/da  = -[y * (1/a)] + [(1 - y) * (1 / 1 - a) * (-1)]\n",
    "    dL/da  = -y/a + ((1 - y) / (1 - a))\n",
    "    \n",
    "    Then,\n",
    "        da = -y/a + ((1 - y) / (1 - a))\n",
    "    \n",
    "`dz`:\n",
    "\n",
    "    All you need to do is:\n",
    "    1 - Replace the values on L(a, y)\n",
    "    2 - Derivate the sigmoid\n",
    "    3 - Remember the chain rule\n",
    "\n",
    "    [...]\n",
    "    \n",
    "    dL/dz = a - y\n",
    "    \n",
    "    Then,\n",
    "        dz = a - y\n",
    "\n",
    "![back1](files/media/back1.png)\n",
    "\n",
    "`dw1`, `dw2` and `db`:\n",
    "\n",
    "    dL/dw1 = dL/dz * dz/dw1 = (a - y) * dz/dw1 = (a - y) * x1 = x1 * \"dz\"\n",
    "    dL/dw1 => \"dw1\" = x1 * \"dz\"\n",
    "\n",
    "    dL/dw2 = dL/dz * dz/dw2 = (a - y) * dz/dw2 = (a - y) * x1 = x2 * \"dz\"\n",
    "    dL/dw2 => \"dw2\" = x1 * \"dz\"\n",
    "            \n",
    "    dL/db = dL/dz * dz/db = (a - y) * dz/db = (a - y) * 1 = 1 * \"dz\" = \"dz\"\n",
    "    dL/db => \"db\" = \"dz\"\n",
    "            \n",
    "![back2](files/media/back2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On 'm' examples (training set):\n",
    "\n",
    "We start `J`, `dw1`, `dw2` and `db` equals to `0`, because we will use them do accumulate their values and then take the average.\n",
    "\n",
    "(Or we can update them on each iteration. e.g.: dw1 /= m)\n",
    "\n",
    "![loss_m](files/media/loss_m.png)\n",
    "\n",
    "\n",
    "Assuming our training examples have a dimmension of 2, hence parameters are only w1, w2:\n",
    "\n",
    "![loss_m-2](files/media/loss_m-2.png)\n",
    "\n",
    "And we update them on each iteration:\n",
    "\n",
    "    J /= m\n",
    "    dw1 /= m\n",
    "    dw2 /= m\n",
    "    db /= m\n",
    "\n",
    "Finally, we take **one step** towards the gradient descent direction, updating our parameters:\n",
    "\n",
    "    w1 = w1 - alfa * dw1\n",
    "    w2 = w2 - alfa * dw2\n",
    "    b = b - alfa * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(10**6)\n",
    "b = np.random.rand(10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product:\n",
    "\n",
    "Same as `scalar product` or `inner product`\n",
    "\n",
    "[![dot](files/media/dot.png)](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjjib-Wk8zmAhVRHLkGHVGwCAoQFjAAegQIARAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FDot_product&usg=AOvVaw133fdk8n2U-mDTqckPu6Aa)\n",
    "(image is a link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 250249.61573987085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Runtime: 3.39 ms'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "c1 = np.dot(a, b)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('c =', c1)\n",
    "'Runtime: {0:.2f} ms'.format((toc - tic)*10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 250249.6157398747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Runtime: 487.11 ms'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = 0\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "for i in range(10**6):\n",
    "    c2 += a[i]*b[i]\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('c =', c2)\n",
    "'Runtime: {0:.2f} ms'.format((toc - tic)*10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vectorize runs 123.33 times faster'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Vectorize runs {0:.2f} times faster'.format(487.16 / 3.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.randint(low=3, size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.zeros((n, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 331.65 ms'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "for i in range(n):\n",
    "    u[i] = math.exp(v[i])\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "'Runtime: {0:.2f} ms'.format((toc - tic)*10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runtime: 16.91 ms'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tic = time.time()\n",
    "\n",
    "u = np.exp(v)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "'Runtime: {0:.2f} ms'.format((toc - tic)*10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vectorize runs 19.61 times faster'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Vectorize runs {0:.2f} times faster'.format(331.65 / 16.91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #### Example 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vec](files/media/vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Foward Propagation:\n",
    "\n",
    "![vec_log](files/media/vec_log.png)\n",
    "\n",
    "In summary, reshape the input vector and features vectors so that we are able to multiply the matrices. (Remember matrix multiplication rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back propagation:\n",
    "\n",
    "![vec_log2](files/media/vec_log2.png)\n",
    "\n",
    "![vec_log3](files/media/vec_log3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fancy word for iteration..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expansion is equal to `(-1) * L(a, z)`. We want to minimize it, to maximize the `ln` of our probability.\n",
    "\n",
    "![logreg_cost](files/media/logreg_cost.png)\n",
    "\n",
    "We get rid of the `-` sign cause we need to minimize `J(w, b)`.\n",
    "\n",
    "If we left the sign, `L` would have to increase ,which means bigger errors.\n",
    "\n",
    "Now, without the negative sign, smaller loss (`L`) will lead to a smaller cost (`J`).\n",
    "\n",
    "![logreg_cost2](files/media/logreg_cost2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
