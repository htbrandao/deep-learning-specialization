{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course1: Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Neural Networks\n",
    "\n",
    "Another example of a neural network:\n",
    "\n",
    "![nn](files/media/nn.png)\n",
    "\n",
    "Neural networks will be stacked as layers, and may be fully connected (dense)\n",
    "\n",
    "![nn2](./files/media/nn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Vectorization\n",
    "\n",
    "Now, follow the foward propagation step on the same behavior which we dealt with on the past week (logistic regression activation)\n",
    "\n",
    "Note that we assign a especial notation (`[i]`) to represent the layer and the input feature\n",
    "\n",
    "![nn3](./files/media/nn3.png)\n",
    "\n",
    "The process will be the same for each neuron on this layer:\n",
    "\n",
    "![nn4](./files/media/nn4.png)\n",
    "\n",
    "What gives us:\n",
    "\n",
    "![nn5](./files/media/nn5.png)\n",
    "\n",
    "We already learned vectorization, so we won't need a `for loop`.\n",
    "\n",
    "Then, \n",
    "\n",
    "![nn6](./files/media/nn6.png)\n",
    "\n",
    "![nn7](./files/media/nn7.png)\n",
    "\n",
    "`a[²] = ŷ`\n",
    "\n",
    "Note that on above example, our input vector x has dimmension equal to 3:\n",
    "\n",
    "    x.shape = (3,) ; x = (ₓ₁,₁ ₓ₁,₂ ₓ₁,₃)\n",
    "    \n",
    "We will be able to apply our network to a entire set X of training examples, vectorizing it\n",
    "\n",
    "Then, we will have X (all input features) where\n",
    "\n",
    "    xᵢ ∈ X, xᵢ = (xᵢ₁,₁ ; xᵢ₂,₁ ; ...)\n",
    "\n",
    "![nn8](./files/media/nn8.png)\n",
    "\n",
    "![nn9](./files/media/nn9.png)\n",
    "\n",
    "![nn10](./files/media/nn10.png)\n",
    "\n",
    "Where each column on **A** refers to each training example. And each line on a given column corresponds to a output from a neuron (hidden **unit**) on the **first** layer. (Note that we are dealing with A[¹])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### On the course, there's a follow up video called \"justification for vectorized implementation\".\n",
    "\n",
    "###### I strongly advise that you watch it carefully in case you are not feeling secure about it!\n",
    "\n",
    "###### We handle **vectors**. So we perform `dot product` instead of matrix multiplication\n",
    "\n",
    "###### In any case, remember the `dot product`:\n",
    "\n",
    "![dot](./files/media/dot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of actvation functions:\n",
    "\n",
    "![af](./files/media/af.png)\n",
    "\n",
    "Try to notice, on each one of them, how the derivative on certain areas  of the curve will be very small, leading our gradient descent to have liitle influence on our training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For example:\n",
    "\n",
    "Suppose we are adjusting our parameters `w`.\n",
    "\n",
    "We would adjust it using a learning rate `a`:\n",
    "\n",
    "    w = w - a * dw\n",
    "\n",
    "Since `dw` could very small - according the the choosen activation function, - it would lead to a small (and probably time consuming) learning/adjustment. `w` will move slower towards the gradient descent, taking more time to minimize the `cost function`.\n",
    "\n",
    "*Which will probably lead to a longer training period with a small learning*\n",
    "\n",
    "###### Note:\n",
    "    \n",
    "    \"dw\" = 0 works better then \"dw\" ~= 0!\n",
    "\n",
    "###### Reminder:\n",
    "\n",
    "    Derivative of 0 (zero):\n",
    "\n",
    "![d0](./files/media/d0.png)\n",
    "\n",
    "###### Great answer about activation functions [here](https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Why non-linear?\n",
    "\n",
    "[...] \"And it turns out that if you use a linear activation function or alternatively, if you don't have an activation function, then no matter how many layers your neural network has, all it's doing is just computing a linear activation function. **So you might as well not have any hidden layers**.\"\n",
    "\n",
    "*(You would be just doing linear combinations of the input features.)*\n",
    "\n",
    "![not](./files/media/not.png)\n",
    "\n",
    "[...] \"if you have a linear activation function here and a sigmoid function here, then this model is no more expressive than standard logistic regression without any hidden layer\"\n",
    "\n",
    "![not2](./files/media/not2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renamed '/home/f4119597/Downloads/Screenshot.png' -> './files/media/not2.png'\r\n"
     ]
    }
   ],
   "source": [
    "!mv -v /home/f4119597/Downloads/Screenshot.png ./files/media/not2.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
