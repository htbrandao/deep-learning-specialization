# Course 1: Week 1

- (x -> ( "neuron" ) -> y  )<=> (y = f(x) )

- ReLU: Rectified Linear Unit

- Dense network: all neurons from the the `i'st layer` receive input from all the neurons on the previous layer (`(i-1)'st layer`)

- 