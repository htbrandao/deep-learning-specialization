# Course 1: Week 1

- x -> ( "neuron" ) -> y = f(x)

- ReLU: Rectified Linear Unit

- Dense network (fully connected): all neurons from the the next layer receives the input from all the neurons from the previous layer

- 